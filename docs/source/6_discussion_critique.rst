.. _discussion-critique:

=======================
6. Discussion Critique
=======================

.. contents::
   :local:
   :depth: 2

---

Synth√®se des R√©sultats
======================

Avant de critiquer, r√©capitulons nos r√©sultats empiriques :

**R√©sultat Principal**

    Corr√©lation de Spearman moyenne entre attention et LIME : :math:`\rho = 0.31`
    
    Cela indique une faible corr√©lation global, insuffisante pour garantir la fiabilit√© de l'attention.

**R√©sultats Secondaires**

- Forte variabilit√© entre phrases (:math:`\rho \in [-0.15, 0.68]`)
- D√©pendance claire du contexte (certaines structures marchent, d'autres non)
- N√©gations syst√©matiquement sous-estim√©es par l'attention

---

Forces de l'Attention comme Explication
========================================

Bien que critique, l'attention a des avantages r√©els.

1. **Rapidit√© de calcul**
   
   L'attention est d√©j√† calcul√©e lors de l'inf√©rence et n'entra√Æne pas de surco√ªt important.
   
   Comparaison :
   
   .. list-table::
      :header-rows: 1
      
      * - M√©thode
        - Temps pour 1000 phrases
      * - Attention
        - ~1 seconde
      * - LIME
        - ~30 minutes (500 samples)
      * - SHAP
        - ~2 heures (combinaisons)
   
   **Avantage** : Ordre de grandeur bien plus rapide.

2. **Interpr√©tabilit√© intuitive**
   
   Les heatmaps d'attention sont faciles √† visualiser et √† comprendre.
   
   Un utilisateur non technique peut rep√©rer rapidement les tokens "regard√©s" par le mod√®le.
   
   Comparaison :
   
   - **Attention** : "Token A a 0.70 d'attention" ‚Üí Clair
   - **LIME** : "Token A a un coefficient -0.23 dans la r√©gression locale" ‚Üí Moins intuitif
   - **SHAP** : "Token A a une valeur Shapley de 0.15" ‚Üí Plus abstrait

3. **Granularit√© d√©taill√©e**
   
   L'attention produit des scores pour :
   
   - Chaque couche (6 niveaux)
   - Chaque t√™te (12 par couche)
   - Chaque position (s√©quence enti√®re)
   
   Il est possible d'analyser les motifs √† chaque niveau.

4. **Insights structurels**
   
   L'attention r√©v√®le des aspects de l'organisation interne du mod√®le :
   
   - Couches basses : relations locales entre tokens (syntaxe)
   - Couches hautes : int√©gration du sens global (s√©mantique)
   
   Cela fournit une fen√™tre sur les repr√©sentations internes.

5. **Absence de perturbation**
   
   Contrairement √† LIME (qui perturbe l'entr√©e), l'attention n'interf√®re pas avec le mod√®le.
   
   ‚Üí Approche non intrusive par rapport au mod√®le.

---

Limitations et Risques de l'Attention
=====================================

Les critiques sont plus graves.

1. **Non-causalit√© fondamentale**
   
   **Probl√®me** : l'attention montre ce que le mod√®le observe, pas n√©cessairement ce qui cause la d√©cision.
   
   **Analogie** :
   
       On demande : ¬´ Pourquoi pensez-vous que c'est dangereux ? ¬ª
       
       R√©ponse (par observation) : ¬´ Je regardais la couleur rouge. ¬ª
       
       Explication causale : ¬´ La couleur rouge indique un risque biologique. ¬ª
   
   En r√©sum√© : l'attention n'explique pas automatiquement le ¬´ pourquoi ¬ª.

2. **Ambigu√Øt√© multi-t√™tes**
   
   Chaque t√™te produit une distribution d'attention diff√©rente.
   
   **Exemple** :
   
   .. code-block:: text
   
       T√™te 1: "good" (0.80)
       T√™te 2: "movie" (0.70)
       T√™te 3: "is" (0.65)
       T√™te 4: "!" (0.82)
       ...
   
   **Question** : laquelle utiliser ? Comment les agr√©ger ?
   
   Pas de consensus standard ‚Üí choix arbitraires.

3. **Biais positionnel**
   
   Les positions initiales et finales re√ßoivent souvent plus d'attention, ind√©pendamment du contenu.
   
   **Test simple** :
   
   .. code-block:: text
   
       Phrase 1: "film good XYZABC" (mot gibberish √† la fin)
       Phrase 2: "film good excellent" (bon mot √† la fin)
       
       ‚Üí L'attention √† la fin peut √™tre similaire.
   
   Il s'agit d'un biais de position, et non d'une mesure d'importance s√©mantique.

4. **Manipulabilit√© et d√©couplage**
   
   **Exp√©rience de Jain & Wallace (2019)** :
   
   Permuter al√©atoirement les poids d'attention d'une phrase peut ne pas changer la pr√©diction.
   
   .. math::
       
       \text{Si} \quad \alpha' \neq \alpha \quad \text{mais} \quad f(x, \alpha') = f(x, \alpha)
       
       \text{Alors l'attention n'est pas n√©cessairement causale}
   
   **Implication** : les poids d'attention peuvent √™tre d√©coupl√©s de la d√©cision finale.

5. **Manque de sp√©cificit√© pour la t√¢che**
   
   L'attention est g√©n√©ralement entra√Æn√©e globalement pour la t√¢che, et non sp√©cifiquement pour chaque classe.
   
   **Exemple** : pour la classification de sentiments
   
   .. code-block:: text
   
       Le m√™me mot "surprising" peut signifier POSITIF ("surprisingly good")
       ou N√âGATIF ("surprisingly bad").
       
       L'attention ne capture pas toujours cette d√©pendance contextuelle.

6. **Instabilit√© et sensibilit√© num√©riques**
   
   La softmax amplifie les petites diff√©rences :
   
   .. math::
       
       \text{score}_1 = 10.0, \quad \text{score}_2 = 9.9 \quad \Rightarrow \quad \alpha_1 = 0.55, \alpha_2 = 0.45
   
   Une petite perturbation (0.1) peut changer l√©g√®rement les poids relatifs.
   
   ‚Üí Risque d'instabilit√© num√©rique.

7. **Agr√©gation arbitraire**
   
   Pour chaque couche et t√™te, on obtient une distribution d'attention distincte.
   
   Comment les combiner ?
   
   - Moyenne, max ou produit ?
   - Moyens de pond√©ration ? Sur quelles bases ?
   
   Chaque strat√©gie d'agr√©gation produit des r√©sultats diff√©rents.

---

Comparaison Empirique : Attention vs LIME vs SHAP
==================================================

Tableau Comparatif Complet
~~~~~~~~~~~~~~~~~~~~~~~~~~

.. list-table::
   :header-rows: 1
   :widths: 15, 12, 12, 12

   * - Crit√®re
     - Attention
     - LIME
     - SHAP
   * - **Co√ªt calcul**
     - Gratuit (inf√©rence)
     - 30 min / 1000
     - 2 heures / 1000
   * - **Fid√©lit√© empirique**
     - Mod√©r√©e (~0.3)
     - Bonne (~0.6)
     - Tr√®s bonne (~0.75)
   * - **Garantie th√©orique**
     - Aucune
     - Locale seulement
     - Fond√©e (Shapley)
   * - **Causalit√©**
     - Non
     - Approximative
     - Valeurs causales
   * - **Stabilit√©**
     - Variable
     - Stochastique
     - D√©terministe
   * - **Interpr√©tabilit√©**
     - Excellente
     - Bonne
     - Moyenne
   * - **Multi-classe**
     - Ambigu
     - Clair
     - Clair
   * - **Utilit√© pratique**
     - Exploration
     - Validation
     - Production

**Conclusions du Tableau** :

- Si **rapidit√©** ‚Üí Attention
- Si **exactitude** ‚Üí SHAP
- Pour **production responsable** ‚Üí LIME + SHAP

---

Cas O√π l'Attention Fonctionne Bien
==================================

L'attention n'est pas inutile. Elle fonctionne dans certains cas.

Cas 1 : Phrases Simples avec Sentiments Explicites
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: text

    "This movie is absolutely fantastic and wonderful!"

Ici, l'attention se concentre sur les adjectifs positifs ("fantastic", "wonderful").

LIME confirme : ces adjectifs sont effectivement les plus importants.

**Corr√©lation** : :math:`\rho = 0.68`

**Pourquoi √ßa marche** :

- Tokens importants sont syntaxiquement/s√©mantiquement explicites
- Pas de negation pour compliquer
- Structure lin√©aire simple

---

Cas 2 : T√¢ches Simples et Claires
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

L'attention fonctionne mieux pour :

- Reconnaissance d'entit√©s nomm√©es (NER)
- Question-r√©ponse (passage pertinent clair)
- Traduction (alignement token-to-token)

Mais √©choue pour :

- Sentiment analysis (n√©cessite compr√©hension contextuelle)
- Inf√©rence logique (sujet d'√©tude)
- Langues morphologiquement complexes

---

Cas O√π l'Attention √âchoue
==========================

Cas 1 : N√©gations
~~~~~~~~~~~~~~~~~

.. code-block:: text

    Phrase A: "This is good"           ‚Üí POSITIF
    Phrase B: "This is not good"       ‚Üí N√âGATIF

Diff√©rence cl√© : un seul mot ("NOT").

**Attention** :

- Pour A : "good" = 0.56
- Pour B : "good" = 0.53, "NOT" = 0.06

L'attention **ne d√©tecte pas** que "NOT" change tout.

**LIME** :

- Pour A : "good" = +0.34
- Pour B : "good" = -0.35, "NOT" = -0.42

LIME capture la d√©pendance au contexte et la n√©gation.

---

Cas 2 : Ambigu√Øt√©s Pragmatiques
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: text

    "I thought I would hate it, but surprisingly I loved it."

Structure complexe :

- "hate" ‚Üí tendance n√©gative, mais neg√© par contexte
- "surprisingly" ‚Üí inversion d'attente
- "loved" ‚Üí sentiment r√©el positif

**Attention** :

Peut se concentrer sur "hate" ou "loved" selon la t√™te de l'attention.

Inconsistent et pas clairement causal.

**LIME** :

Identifie "loved" comme positif et "hate" comme contextuellement n√©gatif (mais neutralis√©).

---

Cas 3 : Double N√©gation
~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: text

    "The movie is not uninteresting"

Double n√©gation = approximativement positif.

**Attention** :

Poids faible pour "not" et "uninteresting" ‚Üí D√©sacord avec la logique.

**LIME** :

Capture la double n√©gation correctement (avec stochastique).

---

Recommandations Pratiques
==========================

Pour les Praticiens
~~~~~~~~~~~~~~~~~~~

1. **Ne pas utiliser l'attention seule comme explication**
   
   Utilisez-la comme outil exploratoire pour d√©boguer et comprendre le mod√®le.

2. **Toujours valider avec LIME ou SHAP**
   
   Avant de publier une explication, validez cette explication avec une m√©thode ind√©pendante.

3. **Transparence**
   
   Si vous utilisez l'attention, informez clairement l'utilisateur :
   
       "Ces heatmaps montrent o√π le mod√®le regarde, pas n√©cessairement pourquoi."

4. **Trier les cas**
   
   - Phrases simples ‚Üí l'attention peut suffire (avec r√©serves)
   - Phrases complexes, n√©gations ‚Üí utiliser LIME/SHAP
   - Production responsable ‚Üí syst√©matiquement LIME/SHAP

5. **Multi-m√©thodes**
   
   Combiner :
   
   - Attention (rapide, intuitive)
   - LIME (locale, empirique)
   - SHAP (th√©orique, globale)
   
   Si les trois m√©thodes concordent ‚Üí confiance accrue.

Pour les Chercheurs
~~~~~~~~~~~~~~~~~~~

1. **D√©velopper des m√©triques de fiabilit√©**
   
   Cr√©er des scores quantitatifs pour pr√©dire quand l'attention est digne de confiance.

2. **Attention am√©lior√©e**
   
   - Attention orient√©e vers la t√¢che (task-aware attention)
   - Attention avec contraintes de causalit√©
   - Attention robuste aux adversaires

3. **Tester sur plus de t√¢ches**
   
   NLP, vision et autres domaines.

4. **Comprendre les failure modes** üêõ
   
   Pourquoi l'attention √©choue-t-elle sur les n√©gations ?
   
   Est-ce l'architecture ? Le donn√©es ? L'optimisation ?

---

Vue Globale : Supporter Jain & Wallace vs Wiegreffe & Pinter
=============================================================

**Jain & Wallace (2019)** : "Attention is Not Explanation"
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Notre √©tude soutient partiellement cette critique.

‚úì **Points valid√©s** :

- Corr√©lation moyenne mod√©r√©e-faible (0.31) avec LIME
- N√©gations mal trait√©es par l'attention
- D√©couplage observ√© entre attention et d√©cision

**Points nuanc√©s** :

- Certains cas pr√©sentent une bonne corr√©lation (œÅ = 0.68)
- L'attention reste utile pour exploration, non pour explication finale
- Distinction importante : ¬´ pas d'explication causale ¬ª n'√©gale pas ¬´ compl√®tement inutile ¬ª

---

**Wiegreffe & Pinter (2019)** : "Attention is Not Not Explanation"
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Notre √©tude soutient partiellement cette r√©ponse.

‚úì **Points soutenant cette critique** :

- Les tests trop stricts de Jain ne refl√®tent pas tous les usages
- L'attention peut aider en contexte (exploration, d√©bugage)
- Distinction entre explication et explication fid√®le importante

**Points de r√©serve** :

- L'attention seule ne suffit pas pour une explication fiable
- Peut √™tre trompeuse si utilis√©e sans discernement
- Requiert validation empirique (LIME/SHAP)

---

Synth√®se : Position Nuanc√©e
~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. note::

    **Notre conclusion** :
    
    L'attention n'est ni une explication compl√®te, ni compl√®tement inutile.
    
    C'est un outil exploratoire puissant qui :
    
    - Offre des insights rapides et visuellement intuitifs
    - Ne garantit pas la causalit√©
    - Peut √™tre trompeuse si mal interpr√©t√©e
    - Reste utile quand valid√©e par d'autres m√©thodes

---

Prochaines √âtapes
==================

Nous concluons avec une synth√®se et des recommandations finales.

Continuez vers la conclusion : :ref:`conclusion-points-cles`

---
