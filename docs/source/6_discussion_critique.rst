.. _discussion-critique:

=======================
6. Discussion Critique
=======================

.. contents::
   :local:
   :depth: 2

---

SynthÃ¨se des RÃ©sultats
======================

Avant de critiquer, rÃ©capitulons nos rÃ©sultats empiriques :

**RÃ©sultat Principal**

    CorrÃ©lation de Spearman moyenne entre attention et LIME : :math:`\rho = 0.31`
    
    Cela indique une faible corrÃ©lation global, insuffisante pour garantir la fiabilitÃ© de l'attention.

**RÃ©sultats Secondaires**

- Forte variabilitÃ© entre phrases (:math:`\rho \in [-0.15, 0.68]`)
- DÃ©pendance claire du contexte (certaines structures marchent, d'autres non)
- NÃ©gations systÃ©matiquement sous-estimÃ©es par l'attention

---

Forces de l'Attention comme Explication
========================================

Bien que critique, l'attention a des avantages rÃ©els.

1. **RapiditÃ© de Calcul** âš¡
   
   L'attention est dÃ©jÃ  calculÃ©e lors de l'infÃ©rence â†’ pas de surcoÃ»t.
   
   Comparaison :
   
   .. list-table::
      :header-rows: 1
      
      * - MÃ©thode
        - Temps pour 1000 phrases
      * - Attention
        - ~1 seconde
      * - LIME
        - ~30 minutes (500 samples)
      * - SHAP
        - ~2 heures (combinaisons)
   
   **Avantage** : Ordre de magnitude plus rapide.

2. **InterprÃ©tabilitÃ© Intuitive** ğŸ¨
   
   Les heatmaps d'attention sont faciles Ã  visualiser et comprendre.
   
   Un utilisateur non-technique peut voir Â« le modÃ¨le regarde ce mot Â» sans calculs complexes.
   
   Comparaison :
   
   - **Attention** : "Token A a 0.70 d'attention" â†’ Clair
   - **LIME** : "Token A a un coefficient -0.23 dans la rÃ©gression locale" â†’ Confus
   - **SHAP** : "Token A a une valeur Shapley de 0.15" â†’ Abstrait

3. **GranularitÃ© DÃ©taillÃ©e** ğŸ”¬
   
   L'attention produit des scores par :
   
   - Chaque couche (6 niveaux)
   - Chaque tÃªte (12 par couche)
   - Chaque position (sÃ©quence entiÃ¨re)
   
   On peut analyser les patterns Ã  chaque niveau.

4. **Insights Structurels** ğŸ§ 
   
   L'attention rÃ©vÃ¨le comment le modÃ¨le organise l'information :
   
   - Couches basses : relationner tokens adjacents (syntaxe)
   - Couches hautes : capturer le sens global (sÃ©mantique)
   
   Cela donne une fenÃªtre sur les reprÃ©sentations internes.

5. **Absence de Perturbation** âœ“
   
   Contrairement Ã  LIME (qui perturbe l'entrÃ©e), l'attention n'interfÃ¨re pas avec le modÃ¨le.
   
   â†’ Plus proche de la vÃ©ritable explication.

---

Limitations et Risques de l'Attention
=====================================

Les critiques sont plus graves.

1. **Non-CausalitÃ© Fondamentale** âš ï¸
   
   **ProblÃ¨me** : L'attention montre ce que le modÃ¨le "observe", pas ce qui *cause* la dÃ©cision.
   
   **Analogue humaine** :
   
       Vous demandez : Â« Pourquoi tu crois que c'est dangereux ? Â»
       
       RÃ©ponse (par "attention") : Â« Je regardais la couleur rouge. Â»
       
       Explication causale rÃ©elle : Â« La couleur rouge signale un risque biologique. Â»
   
   L'attention n'explique pas le "pourquoi".

2. **AmbiguÃ¯tÃ© Multi-TÃªtes** ğŸ­
   
   Chaque tÃªte produit une distribution d'attention diffÃ©rente.
   
   **Exemple** :
   
   .. code-block:: text
   
       TÃªte 1: "good" (0.80)
       TÃªte 2: "movie" (0.70)
       TÃªte 3: "is" (0.65)
       TÃªte 4: "!" (0.82)
       ...
   
   **Question** : Laquelle prendre ? Comment agrÃ©ger ?
   
   Pas de consensus standard â†’ choix arbitraires.

3. **Biais Positionnel** ğŸ“
   
   Les positions initiales et finales reÃ§oivent souvent plus d'attention, indÃ©pendamment du contenu.
   
   **Test simple** :
   
   .. code-block:: text
   
       Phrase 1: "film good XYZABC" (mot gibberish Ã  la fin)
       Phrase 2: "film good excellent" (bon mot Ã  la fin)
       
       â†’ L'attention Ã  la fin peut Ãªtre similaire !
   
   C'est un **biais de position**, pas d'importance sÃ©mantique.

4. **ManipulabilitÃ© et DÃ©couplage** ğŸª
   
   **ExpÃ©rience de Jain & Wallace (2019)** :
   
   Permuter alÃ©atoirement les poids d'attention d'une phrase ne change pas la prÃ©diction.
   
   .. math::
       
       \text{Si} \quad \alpha' \neq \alpha \quad \text{mais} \quad f(x, \alpha') = f(x, \alpha)
       
       \text{Alors l'attention n'est pas causale}
   
   **Implication** : Les poids d'attention sont **dÃ©cuplÃ©s** de la dÃ©cision rÃ©elle.

5. **Manque de SpÃ©cificitÃ© pour la TÃ¢che** ğŸ¯
   
   L'attention est entraÃ®nÃ©e globalement sur la tÃ¢che, pas spÃ©cifiquement pour chaque classe.
   
   **Exemple** : Pour la classification de sentiments
   
   .. code-block:: text
   
       MÃªme mot "surprising" peut signifier POSITIF ("surprisingly good")
       ou NÃ‰GATIF ("surprisingly bad")
       
       L'attention ne capture pas cette dÃ©pendance au contexte de classe.

6. **InstabilitÃ© et SensibilitÃ© NumÃ©riques** ğŸ”€
   
   La softmax amplifie les petites diffÃ©rences :
   
   .. math::
       
       \text{score}_1 = 10.0, \quad \text{score}_2 = 9.9 \quad \Rightarrow \quad \alpha_1 = 0.55, \alpha_2 = 0.45
   
   Une petite perturbation de 0.1 change le classement de 5%.
   
   â†’ InstabilitÃ© numÃ©rique.

7. **AgrÃ©gation Arbitraire** ğŸ”§
   
   Pour chaque couche et tÃªte, on obtient une attention diffÃ©rente.
   
   Comment les combiner ?
   
   - Moyenne ? Max ? Produit ?
   - Poids par importance ? BasÃ© sur quoi ?
   
   Chaque choix donne des rÃ©sultats diffÃ©rents.

---

Comparaison Empirique : Attention vs LIME vs SHAP
==================================================

Tableau Comparatif Complet
~~~~~~~~~~~~~~~~~~~~~~~~~~

.. list-table::
   :header-rows: 1
   :widths: 15, 12, 12, 12

   * - CritÃ¨re
     - Attention
     - LIME
     - SHAP
   * - **CoÃ»t calcul**
     - Gratuit (infÃ©rence)
     - 30 min / 1000
     - 2 heures / 1000
   * - **FidÃ©litÃ© empirique**
     - ModÃ©rÃ©e (~0.3)
     - Bonne (~0.6)
     - TrÃ¨s bonne (~0.75)
   * - **Garantie thÃ©orique**
     - Aucune
     - Locale seulement
     - FondÃ©e (Shapley)
   * - **CausalitÃ©**
     - Non
     - Approximative
     - Valeurs causales
   * - **StabilitÃ©**
     - Variable
     - Stochastique
     - DÃ©terministe
   * - **InterprÃ©tabilitÃ©**
     - Excellente
     - Bonne
     - Moyenne
   * - **Multi-classe**
     - Ambigu
     - Clair
     - Clair
   * - **UtilitÃ© pratique**
     - Exploration
     - Validation
     - Production

**Conclusions du Tableau** :

- Si **rapiditÃ©** â†’ Attention âœ“
- Si **exactitude** â†’ SHAP âœ“
- Pour **production responsable** â†’ LIME + SHAP âœ“

---

Cas OÃ¹ l'Attention Fonctionne Bien
==================================

L'attention n'est pas inutile. Elle fonctionne dans certains cas.

Cas 1 : Phrases Simples avec Sentiments Explicites
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: text

    "This movie is absolutely fantastic and wonderful!"

Ici, l'attention se concentre sur les adjectifs positifs ("fantastic", "wonderful").

LIME confirme : ces adjectifs sont effectivement les plus importants.

**CorrÃ©lation** : :math:`\rho = 0.68` âœ“

**Pourquoi Ã§a marche** :

- Tokens importants sont syntaxiquement/sÃ©mantiquement explicites
- Pas de negation pour compliquer
- Structure linÃ©aire simple

---

Cas 2 : TÃ¢ches Simples et Claires
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

L'attention fonctionne mieux pour :

- Reconnaissance d'entitÃ©s nommÃ©es (NER)
- Question-rÃ©ponse (passage pertinent clair)
- Traduction (alignement token-to-token)

Mais Ã©choue pour :

- Sentiment analysis (nÃ©cessite comprÃ©hension contextuelle)
- InfÃ©rence logique (sujet d'Ã©tude)
- Langues morphologiquement complexes

---

Cas OÃ¹ l'Attention Ã‰choue
==========================

Cas 1 : NÃ©gations
~~~~~~~~~~~~~~~~~

.. code-block:: text

    Phrase A: "This is good"           â†’ POSITIF
    Phrase B: "This is not good"       â†’ NÃ‰GATIF

DiffÃ©rence clÃ© : un seul mot ("NOT").

**Attention** :

- Pour A : "good" = 0.56 âœ“
- Pour B : "good" = 0.53, "NOT" = 0.06 âœ—

L'attention **ne dÃ©tecte pas** que "NOT" change tout.

**LIME** :

- Pour A : "good" = +0.34
- Pour B : "good" = -0.35, "NOT" = -0.42 âœ“

LIME capture la dÃ©pendance au contexte et la nÃ©gation.

---

Cas 2 : AmbiguÃ¯tÃ©s Pragmatiques
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: text

    "I thought I would hate it, but surprisingly I loved it."

Structure complexe :

- "hate" â†’ tendance nÃ©gative, mais negÃ© par contexte
- "surprisingly" â†’ inversion d'attente
- "loved" â†’ sentiment rÃ©el positif

**Attention** :

Peut se concentrer sur "hate" ou "loved" selon la tÃªte de l'attention.

Inconsistent et pas clairement causal.

**LIME** :

Identifie "loved" comme positif et "hate" comme contextuellement nÃ©gatif (mais neutralisÃ©).

---

Cas 3 : Double NÃ©gation
~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: text

    "The movie is not uninteresting"

Double nÃ©gation = approximativement positif.

**Attention** :

Poids faible pour "not" et "uninteresting" â†’ DÃ©sacord avec la logique.

**LIME** :

Capture la double nÃ©gation correctement (avec stochastique).

---

Recommandations Pratiques
==========================

Pour les Praticiens
~~~~~~~~~~~~~~~~~~~

1. **Ne pas utiliser l'attention seule comme explication** âŒ
   
   Utilisez-la comme **outil exploratoire** pour dÃ©boguer et comprendre le modÃ¨le.

2. **Toujours valider avec LIME ou SHAP** âœ“
   
   Avant de publier une explication, validez avec une mÃ©thode indÃ©pendante.

3. **Transparence** ğŸ¯
   
   Si vous utilisez l'attention, dites clairement Ã  l'utilisateur :
   
       "Ces heatmaps montrent oÃ¹ le modÃ¨le regarde, pas nÃ©cessairement pourquoi."

4. **Trier les cas** ğŸ“Š
   
   - Phrases simples â†’ Attention peut suffire (avec caveats)
   - Phrases complexes, nÃ©gations â†’ Utilisez LIME/SHAP
   - Production responsable â†’ Always LIME/SHAP

5. **Multi-MÃ©thodes** ğŸ”„
   
   Croiser :
   
   - Attention (rapide, intuitive)
   - LIME (locale, empirique)
   - SHAP (thÃ©orique, globale)
   
   Si les trois concordent â†’ confidence Ã©levÃ©e.

Pour les Chercheurs
~~~~~~~~~~~~~~~~~~~

1. **DÃ©velopper des mÃ©triques de fiabilitÃ©** ğŸ”¬
   
   CrÃ©er des scores quantitatifs pour quand l'attention est trustworthy.

2. **Attention amÃ©liorÃ©e** ğŸš€
   
   - Attention orientÃ©e vers la tÃ¢che (task-aware attention)
   - Attention avec contraintes de causalitÃ©
   - Attention robuste aux adversaires

3. **Tester sur plus de tÃ¢ches** ğŸ§ª
   
   NLP (rÃ©cente), Vision (important), Autres domaines.

4. **Comprendre les failure modes** ğŸ›
   
   Pourquoi l'attention Ã©choue-t-elle sur les nÃ©gations ?
   
   Est-ce l'architecture ? Le donnÃ©es ? L'optimisation ?

---

Vue Globale : Supporter Jain & Wallace vs Wiegreffe & Pinter
=============================================================

**Jain & Wallace (2019)** : "Attention is Not Explanation"
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Notre Ã©tude soutient partiellement cette critique.

âœ“ **Points confirmÃ©s** :

- CorrÃ©lation moyenne faible (0.31) avec LIME
- NÃ©gations mal traitÃ©es
- DiscovÃ©lage entre attention et dÃ©cision

âš  **Points Ã  nuancer** :

- Certains cas marchent bien (Ï = 0.68)
- Attention utile pour exploration, pas pour explication finale
- DiffÃ©rence entre "pas d'explication causale" et "pas d'explication du tout"

---

**Wiegreffe & Pinter (2019)** : "Attention is Not Not Explanation"
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Notre Ã©tude soutient partiellement cette rÃ©ponse.

âœ“ **Points confirmÃ©s** :

- Les tests trop stricts de Jain ne reflÃ¨tent pas tous les usages
- Attention peut aider en contexte (exploration, dÃ©bugage)
- Distinction entre explication et explication fidÃ¨le importante

âš  **Cependant** :

- L'attention seule **ne suffit pas** pour une explication fidÃ¨le
- Peut Ãªtre tromperie si utilisÃ©e naÃ¯vement
- Requiert validation empirique (LIME/SHAP)

---

SynthÃ¨se : Position NuancÃ©e
~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. note::

    **Notre conclusion** : 
    
    L'attention n'est ni une explication complÃ¨te, ni complÃ¨tement inutile.
    
    C'est un **outil exploratoire puissant** qui :
    
    - âœ“ Offre des insights rapides et visuellement intuitifs
    - âœ— Ne garantit pas la causalitÃ©
    - âš  Peut Ãªtre trompeuse si mal interprÃ©tÃ©e
    - âœ“ Reste utile quand validÃ©e par d'autres mÃ©thodes

---

Prochaines Ã‰tapes
==================

Nous concluons avec une synthÃ¨se et des recommendations finales.

.. button-ref:: 7_conclusion_points_cles
   :color: primary
   :outline:

   Vers la Conclusion â†’

---
