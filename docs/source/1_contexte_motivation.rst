.. _contexte-motivation:

============================
1. Contexte & Motivation
============================

.. contents::
   :local:
   :depth: 2

---

Le Probl√®me Fondamental
========================

Les mod√®les **Transformer** modernes (BERT, GPT, RoBERTa, etc.) utilisent un m√©canisme appel√© **Self-Attention** pour traiter et analyser les s√©quences textuelles.

Ce m√©canisme produit des **poids d'attention** : des probabilit√©s indiquant le "degr√© d'int√©r√™t" du mod√®le pour chaque token de l'entr√©e.

.. note::
   
   **En th√©orie** : Ces poids d'attention devraient r√©v√©ler quels tokens influencent la pr√©diction du mod√®le.
   
   **En pratique** : Cela ne fonctionne pas toujours !

---

Le D√©bat Scientifique : Deux Visions Oppos√©es
==============================================

En 2019, deux articles majeurs ont lanc√© un d√©bat acad√©mique intense sur la fiabilit√© de l'attention comme outil d'explication.

Article 1 : ¬´ Attention is Not Explanation ¬ª
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Auteurs** : Sarthak Jain & Byron C. Wallace  
**Conf√©rence** : NAACL 2019  
**ArXiv** : https://arxiv.org/abs/1902.10186

**Th√®se principale** :

    Les poids d'attention ne constituent **pas** des explications fiables des d√©cisions du mod√®le.

**Arguments cl√©s** :

1. **Test de permutation** : Permuter al√©atoirement les poids d'attention ne change souvent pas la pr√©diction
   
   .. math::
      
      \text{Si} \quad \sigma(\alpha) = \sigma(\alpha') \quad \text{mais} \quad \alpha \neq \alpha'
      
      \text{Alors l'attention ne capture pas les d√©cisions causales}

2. **Distributions alternatives** : D'autres distributions de poids produisent des r√©sultats identiques

3. **Faible corr√©lation gradient** : Corr√©lation faible entre attention et gradients (mesure √©tablie d'importance)

4. **M√©taphore du "o√π" vs "pourquoi"** : L'attention montre **o√π** le mod√®le regarde, pas **pourquoi** il d√©cide

**Impact** : Article tr√®s influent ayant sem√© le doute sur la validit√© de visualiser les heatmaps d'attention.

---

Article 2 : ¬´ Attention is Not Not Explanation ¬ª
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Auteurs** : Sarah Wiegreffe & Yuval Pinter  
**Conf√©rence** : EMNLP 2019  
**ArXiv** : https://arxiv.org/abs/1908.04626

**Th√®se principale** :

    L'attention **peut** constituer une explication valide, sous certaines conditions et interpr√©tations appropri√©es.

**Contre-arguments cl√©s** :

1. **Critique m√©thodologique** : Les tests de Jain & Wallace sont trop stricts et artificiels

2. **D√©pendance contextuelle** : L'interpr√©tabilit√© d√©pend du type de mod√®le, de t√¢che et de donn√©es

3. **Explication vs Explication fid√®le** : Distinction importante entre :
   
   - **Explication** : Attribution de causalit√© logique
   - **Explication fid√®le** : Attribution math√©matiquement garantie
   
   L'attention peut √™tre une explication sans √™tre une explication fid√®le.

4. **Validation empirique** : Certains cas montrent une bonne corr√©lation entre attention et importance r√©elle

**Impact** : Apporte une nuance importante : le probl√®me n'est pas binaire, mais d√©pend fortement du contexte.

---

O√π se Situe ce Projet ?
~~~~~~~~~~~~~~~~~~~~~~~

Ce projet **prend du recul** et propose une **analyse critique autonome** :

.. image:: _static/debate_spectrum.txt
   :align: center
   :alt: Spectrum du d√©bat

- ‚úó Nous ne d√©fendons pas l'une ou l'autre position absolue
- ‚úì Nous **reproduisons exp√©rimentalement** les critiques des deux camps
- ‚úì Nous **quantifions empiriquement** la fiabilit√© de l'attention via LIME et SHAP
- ‚úì Nous **d√©veloppons un jugement critique** nuanc√©

---

Objectifs du Projet
====================

| Objectif | Description | M√©trique |
|:---------|:-----------|----------|
| **Analyser** | Examiner si l'attention capture l'importance r√©elle | Corr√©lation de Spearman |
| **Comparer** | Confronter attention, LIME et SHAP | Visualisations comparatives |
| **D√©montrer** | Identifier des cas pathologiques | Cas d'√©tude (n√©gations, etc.) |
| **Critiquer** | D√©velopper un jugement nuanc√© | Discussion qualitative |

---

Questions de Recherche
======================

Cette √©tude tente de r√©pondre √† :

1. **Q1** : Quelle est la corr√©lation entre les poids d'attention et les mesures d'importance √©tablies (LIME, SHAP) ?

2. **Q2** : L'attention g√®re-t-elle correctement les structures linguistiques complexes (n√©gations, conjonctions) ?

3. **Q3** : Existe-t-il des heuristiques pour identifier quand l'attention est fiable vs trompeuse ?

4. **Q4** : Comment utiliser l'attention de mani√®re responsable dans une pipeline XAI ?

---

Caract√©ristiques de l'√âtude
============================

.. list-table::
   :header-rows: 1

   * - Propri√©t√©
     - Valeur
   * - **Type d'explication**
     - Locale (per-instance)
   * - **Domaine**
     - NLP / Analyse de sentiments
   * - **Mod√®le √©tudi√©**
     - DistilBERT fine-tun√© (SST-2)
   * - **T√¢che**
     - Classification binaire (Positif/N√©gatif)
   * - **Sortie**
     - Scores d'importance par token
   * - **Famille XAI**
     - Gradient-free + Attention-based
   * - **Nombre de cas**
     - 7+ phrases de test
   * - **M√©thodes de validation**
     - LIME, SHAP, corr√©lation de Spearman

---

Enjeux Pratiques
=================

Pourquoi cette question est-elle importante ?

1. **Trustworthiness** üîê
   
   Si l'attention est trompeuse, visualiser des heatmaps augmente faussement la confiance des utilisateurs.

2. **Responsabilit√©** ‚öñÔ∏è
   
   Les syst√®mes de recommandation ou de classification doivent reposer sur des explications **r√©elles**, pas superficielles.

3. **Recherche** üî¨
   
   Le d√©bat affecte comment on interpr√®te les r√©sultats des mod√®les Transformer.

4. **Adoption** üìà
   
   Savoir quand faire confiance √† l'attention guidera son utilisation en production.

---

Contribution du Projet
======================

Ce projet contribue :

‚úì **Reproduction** des critiques scientifiques dans un cadre unifi√©  
‚úì **Validation empirique** sur des cas fran√ßais et anglais  
‚úì **Identification** de points de basculement (quand l'attention faillit)  
‚úì **Recommandations** pratiques pour l'usage responsable  
‚úì **Code ouvert** pour √©tudier d'autres mod√®les/t√¢ches  

---

Structure de la Suite
=====================

La documentation progresse comme suit :

1. **Intuition** ‚Üí Explication conceptuelle facile √† comprendre
2. **Th√©orie** ‚Üí Formalismes math√©matiques rigoureux
3. **Impl√©mentation** ‚Üí Code reproductible et d√©taill√©
4. **R√©sultats** ‚Üí Exp√©riences et visualisations
5. **Critique** ‚Üí Analyse nuanc√©e et recommandations
6. **Conclusion** ‚Üí Synth√®se et perspectives

---

Pr√©requis
=========

Pour suivre ce projet, il est utile de conna√Ætre :

- **NLP de base** : Tokens, embeddings, transformer
- **Python** : Pandas, NumPy, PyTorch
- **Visualisation** : Matplotlib, Seaborn
- **Statistiques** : Corr√©lation, p-values
- **XAI** : LIME, SHAP (br√®ves explications fournies)

.. note::
   
   Pas de panique ! Chaque concept est expliqu√© progressivement.

---

Prochaines √âtapes
==================

Pr√™t √† plonger ? Commencez par :

1. **Lire** l'intuition de la m√©thode (section 2)
2. **Ma√Ætriser** la formalisation math√©matique (section 3)
3. **Ex√©cuter** le code (section 4)
4. **Analyser** les r√©sultats (section 5)

.. button-ref:: 2_intuition_methode
   :color: primary
   :outline:

   Continuer vers l'Intuition ‚Üí

---
