# XAI Mini-Projet : Attention is Not Explanation - Read the Docs

Documentation complÃ¨te basÃ©e sur le notebook Jupyter `Projet7_Attention_Not_Explanation.ipynb`.

## ðŸ“š Ã€ Propos

Ce projet est une Ã©tude empirique combinant recherche acadÃ©mique et implÃ©mentation pratique pour rÃ©pondre Ã  une question fondamentale en explicabilitÃ© de l'IA :

**Les poids d'attention des Transformers constituent-ils de vÃ©ritables explications des dÃ©cisions du modÃ¨le ?**

## ðŸŽ¯ Contenu

La documentation est organisÃ©e en 8 sections principales :

1. **Contexte & Motivation** - DÃ©bat scientifique entre Jain & Wallace (2019) et Wiegreffe & Pinter (2019)
2. **Intuition de la MÃ©thode** - Explication conceptuelle du mÃ©canisme d'attention
3. **Formalisation MathÃ©matique** - Ã‰quations et mÃ©triques rigoureuses
4. **ImplÃ©mentation Pratique** - Guide d'installation et code reproductible
5. **ExpÃ©riences & Visualisations** - RÃ©sultats empiriques et analyses
6. **Discussion Critique** - Forces, limitations et recommandations
7. **Conclusion & Points ClÃ©s** - SynthÃ¨se et implications pratiques
8. **RÃ©fÃ©rences** - Bibliographie complÃ¨te (12+ articles majeurs)

## ðŸ”§ PrÃ©-requis Techniques

### Pour Jupyter (Notebook)
```bash
pip install torch transformers lime shap matplotlib seaborn pandas numpy scipy
```

### Pour Sphinx (Documentation)
```bash
pip install sphinx sphinx-rtd-theme
```

## ðŸ“– Lectures RecommandÃ©es

### DÃ©butant (30 minutes)
1. Cette README
2. Section "Intuition de la MÃ©thode" (Index â†’ 2)
3. FAQ

### IntermÃ©diaire (2-3 heures)
1. Sections 1-5 complÃ¨tes
2. Glossaire pour clarifications
3. RÃ©fÃ©rences pour les articles clÃ©s

### AvancÃ© (1 jour+)
1. Toute la documentation
2. Relire le notebook Jupyter
3. Reproduire les expÃ©riences
4. Lire les 12 articles de rÃ©fÃ©rence

## ðŸŽ“ Apprenants Cibles

âœ“ **Ã‰tudiants** en IA/ML : Comprendre XAI  
âœ“ **Data Scientists** : ImplÃ©menter LIME/SHAP responsablement  
âœ“ **Chercheurs** : Investiguer explicabilitÃ© de l'IA  
âœ“ **Product Managers** : DÃ©cider quand utiliser l'attention  
âœ“ **Policy Makers** : RÃ©glementer l'IA explicable  

## ðŸ” Points ClÃ©s

### Forces de l'Attention
- âš¡ TrÃ¨s rapide (gratuit Ã  infÃ©rence)
- ðŸŽ¨ Facile Ã  visualiser et interprÃ©ter
- ðŸ”¬ GranularitÃ© dÃ©taillÃ©e (couches, tÃªtes, tokens)
- ðŸ§  RÃ©vÃ¨le les patterns internes du modÃ¨le

### Limitations de l'Attention
- âŒ Non causale (observation â‰  causalitÃ©)
- ðŸŽ­ AmbiguÃ¯tÃ© multi-tÃªtes
- ðŸ“ Bias positionnel
- ðŸš« Ã‰choue sur les nÃ©gations
- ðŸŽª Manipulable (permutation sans effet)

## ðŸ’¡ Recommandations Pratiques

### Pipeline XAI Responsable
```
1. Extraction    â†’ Attention + LIME + SHAP
2. Validation    â†’ CorrÃ©lation Spearman > 0.5 ?
3. Classement    â†’ Simple vs Complexe
4. Affichage     â†’ Attention (simple) ou LIME/SHAP (complexe)
5. Audit         â†’ Tests adversariaux
```

### Quand Utiliser Quoi
| Contexte | Attention | LIME | SHAP |
|----------|:---------:|:----:|:----:|
| DÃ©bugage rapide | âœ“ | - | - |
| Exploration | âœ“ | âœ“ | - |
| Validation | - | âœ“ | âœ“ |
| Production sensible | - | âœ“ | âœ“ |
| ConformitÃ© GDPR | âš  | âœ“ | âœ“ |


## ðŸ”— Ressources Externes

### Articles ClÃ©s
- Jain & Wallace (2019) - "Attention is Not Explanation"
- Wiegreffe & Pinter (2019) - "Attention is Not Not Explanation"
- Vaswani et al. (2017) - "Attention is All You Need"

### Outils
- HuggingFace Transformers : https://huggingface.co/transformers/
- SHAP : https://shap.readthedocs.io/
- LIME : https://github.com/marcotcr/lime

### Cours
- Stanford CS224N : https://cs224n.stanford.edu/
- HuggingFace Course : https://huggingface.co/course


## ðŸŽ‰ Remerciements

Merci Ã  :
- Jain & Wallace pour avoir soulevÃ© la question
- Wiegreffe & Pinter pour avoir nuancÃ© le dÃ©bat
- La communautÃ© NLP/XAI pour les discussions continues

---

**DerniÃ¨re mise Ã  jour** : DÃ©cembre 2025  
**Version** : 1.0  
**Statut** : Complet et reproductible
